## Table of Contents

- 1 Introduction
- 2 Related Work
   - 2.1 Text Interpretation
   - 2.2 Natural Language Processing
   - 2.3 Text Visualization
- 3 Visualizing Interpretation
   - 3.1 Relevance
   - 3.2 Text Visualization Workshop
   - 3.3 Design Proposal
      - 3.3.1 Outline
      - 3.3.2 Design Process
   - 3.4 Prototypical Iterations
- 4 Discussion
- 5 Conclusion


“[Analysis] merely consists in composing and decomposing our
ideas to create new combinations and to discover, by this means,
their mutual relations and the new ideas they can produce.”
    - Etienne Bonnot de Condillac (2001)

## 1 Introduction

**How does a digital environment look like that visually augments the process
of text interpretation?**

Bruno Latour (1986) already observed the privileged position of paper and text
for communicating scientific results and discussion. All results from calculations,
observations and build prototypes were distilled in narrated forms of text to
convey ideas. This is no different today as a wide range of disciplines still use text
as the primary exchange format (Gascoigne et al., 2020).


## 2 Related Work

### 2.1 Text Interpretation

The practice of text interpretation has a rich history of philosophies and techniques
that influenced the process of getting meaning out of texts. This field of interest is
often referred to as “hermeneutics” (Figal, 2007). The next two sections will give a
short introduction to premises of the hermeneutics and the techniques which are
used to analyze texts.

Hermeneutics, from the Greek word “hermeneu‘s” – mediation, is known as the
philosophy of interpretation and understanding and has a long history with an ever-
changing understanding of the interpretation of natural language (Figal, 2007).
Although historians traced the beginnings of hermeneutics to antiquity,
fundamental alterations started to occur in the 15th century, which largely influenced
the following centuries. In these times the literacy among the population was scarce
and mostly people that belonged to religious or judicial institutions were able to
read and write. Therefore, hermeneutics was mainly concerned with religious and
judicial texts and how to correctly interpret them. During antiquity and till the 15th
century it was established that texts are rich in a plurality of interpretations which
enabled a rich discourse around its meaning.

This changed in the 16th century as Martin Luther proposed a return to a literal
understanding of the biblical texts which according to him “interprets itself” and
does not need human intervention. Due to this understanding, hermeneutics became
part of logic and the textual rules, such as rhetoric, grammar and philology,
provided the framework to uncover the only true meaning already inscribed into the
text. During the times of the enlightment, most prominently Immanuel Kant, puts
the subject at the center of experience and the potential for understanding (Weimar,
2007). In the tradition of Kant, the theologist Friedrich Schleiermacher (1838) also
includes a psychological understanding of the text which takes the authors
intentions into account and combines this with the linguistic analysis.





Schleiermacher is also the first to propose to open the interpretation practice to all
sorts of texts which was previously still restricted to religious and judicial texts. He
also argues for an intuitive understanding of the interpreting subject, “divination”,
which arises during the process. Schleiermacher also understood the process of
understanding as iterative, where prior knowledge would inform the next iteration
of interpretation, also known as the hermeneutical circle.

The thoughts of Schleiermacher are extended by Wilhelm Dilthey (1900) who also
considers the societal and historic context of the author into the interpretation of the
text. Dilthey also extends on the concepts regarding the hermeneutical circle from
Schleiermacher and sees the individual experiences and expressions in the historic
contexts and these contexts as manifestations of the variety of experiences and
expressions from individuals. Lastly Hans-Georg Gadamer (1975) extended the
hermeneutical circle by the so called “fusion of horizons”. This process describes
the fusion of the contextual horizons of the reader, text and author, which generates
new understanding through the tension of the contexts.

In conclusion, the theory and practice of interpretation has changed substantially
over the last centuries. From an understanding of texts as isolated objects to an
understanding of texts as constructed by the historical and societal contexts of
author and recipient. Additionally, the latest contributions by Gadamer greatly
changed the context and academic position of hermeneutics, shifting it back to a
concern of philosophy. In this regard the experiencing subject is at the center of the
interpretation with its personal prejudices, societal and historic contexts which have
a great influence on this process.

The techniques, which are used for text analysis and interpretative purposes, have
been developed to investigate texts upon the described premises, foster deeper
comprehension and enable critique. A widely acknowledged technique in the
humanities for text analysis is called _close reading_.


**Close Reading**
One outstanding work for the introduction of close reading is the book “How to
Read a Book” by Mortimer J. Adler and his editor Charles van Doren (Adler & Van
Doren, 1972). In their work Adler and van Doren defined several distinct modes of
reading which foster deeper comprehension of texts and allow the reader to critique
the claims and assumptions of the author. Although the book is quite old, two
chapters are still relevant to the contemporary close reading practice: _inspectional
reading_ and _analytical reading_. Inspectional reading is aimed at the goal of gaining
an informed overview over the structure of the work and its ideas, called _skimming_.
Afterwards, the text is read at once without stopping for selective analysis of
passages, to grasp the overall argument structure. The inspectional reading equips
the reader with an overview that enables him to conduct an in-depth analytical
reading of the text. The goal of the analytical reading is the informed capability to
virtually dialogue with the author and his arguments by critiquing his work. As a
means for gaining this capability, analytical reading examines the genre of the text,
understanding the questions that the author prompts and _scanning_ for keywords and
sentences with heightened importance. James Jasinski (2001) has distilled the
method of analytical reading into four main aspects that can be analyzed:
individuals and events, words and phrases, structure and style and argument pattern.
The knowledge about these elements of the text allows the reader to find different
angles from where the text and its statements can be challenged.

Close reading became common in academic circles and prominent figures like
Michel Foucault or Fredric Jameson were pushing the ideas of a close reading as
_symptomatic reading_ which tries to uncover the ideologies and hidden forces that
shape texts and other forms of media (Best & Marcus, 2009). In this regard Nancy
Boyles (2013) describes close reading as a “reading to uncover layers of meaning
that lead to deep comprehension” (p.1). Although this type of reading is much
appreciated for its critical and suspicious approach to texts, Best and Marcus (2009)
also argue to return to a more concrete form of _close reading_ which they call _surface
reading._ They argue to come back to the information that the text itself provides
and interpret what is given, not what is suspected by cryptic chains of inference.





Therefore, a mix of these forms of _close reading_ can be used to analyze the text on
different levels.

Most academic readers (also students) go multiple times over the same text to
investigate different aspects and use the knowledge gained for inference (Guillory,
2008). As this process is iterative but also incremental the hermeneutical circle (or
rather spiral) is often used as a metaphor for the process. To support the process of
active reading, annotations in the form of comments or visual markings (see Figure
1 ) are often used for conversational engagements with the text. Kalir and Garcia
(2021) observe that annotations “can be a way to create, document, and curate new
knowledge” and can be seen as “a form of self-expression” (p.7). Therefore,
annotations help to explicitly describe context, capture thoughts and prompt
questions for the purpose of an individualized process of text comprehension.


Figure 1 : Manual annotation of a text (Figure reproduced with permission from Kehoe
and Gee (2013))


**Hyper Reading**
In the 1960s Ted Nelson, a sociologist interested in network technologies,
introduced the concept of the “hypertext” which he described as the following:


Let me introduce the word "hypertext" to mean a body of written
or pictorial material interconnected in such a complex way that it
could not conveniently be presented or represented on paper. It
may contain summaries, or maps of its contents and their
interrelations; it may contain annotations, additions and footnotes
from scholars who have examined it. (Nelson, 1965, p. 96)

Here a more engaging introduction could be made to the possibilities of hypertext
and hyperreading
On the brink of the new millennial literary scholar James Sosnoski (1999) observed
several behavioral changes in reading amongst scholars and students which were
strongly connected to hypertext-based reading. He loosely defined these changes as
_hyper-reading_ , which is defined by eight characteristic techniques connected to the
hypertext medium: filtering, skimming, pecking, imposing, filming, trespassing,
de-authorizing and fragmenting. These changes were also observed by other
scholars from the humanities which prompted questions about the future of reading
and interpretation in times of increased cognitive load and distraction (Guillory,
2008; Hayles, 2010). John Guillory (2008) emphasizes especially two techniques,
which dominate the reading practice amongst academics: _scanning_ and _skimming._
Scanning enables the reader to quickly assert if a text is relevant to the topic he is
currently interested in by looking for keywords or semantically related information.
This is supported by the _filtering_ technique enabled by keyword-based interactive
searches. Skimming is used to quickly grasp the main points of a text and make
inferential statements about it without going through the whole text. Both
techniques deal with the increased richness in information and the exponential
increasing quantity of eligible texts, which is empirically connected to cognitive
overload (DeStefano & LeFevre, 2007). This is underlined by the famous eye-
tracking study of Jakob Nielsen (2006) which provided the insight to the “F-





shaped” pattern which users follow on visiting a website, that indicates the
reduction of cognitive load by employing the prior mentioned _scanning_ technique.

### 2.2 Natural Language Processing

“Natural language processing is the set of methods for making human language
accessible to computers” (Eisenstein, 2019, p. 1). As the advent of the digital
computer is younger than the history of information sciences and linguistics, the
field of natural language processing (NLP) draws its methodologies and
inspirations from several other fields such as physics and mathematics but also
more distinct ones like computational linguistics and machine learning (Eisenstein,
2019). This field uses different techniques and algorithms to extract information out
of texts and the ones of interest to this work are presented in the following section.

Look over this https://web.stanford.edu/~jurafsky/slp3/

**Regular Expressions**
To find a text by a pattern one must be able to describe such pattern by a formal
description of elements in the text which are previously unknown exactly (Manning
& Schütze, 1999). Regular expressions are the go-to method for any computer
program that deals with search functions. An example: in the sentence “Mary eats
ice” one can extract each word by the expression “\w*” where “\w” indicates to
match any word and “*” to match zero or more. This can be applied to every type
of string to extract all words in that text.

**N-Grams**
The modeling of n-grams is a central modeling technique for creating a structured
corpus out of unstructured text. Manning and Schütze (1999) describe this under
the topic of entropy in language and the possibility of combinations of words as a
probabilistic method. This is also sometimes referred to as a markov-chain. In NLP
this is applied in the form of splitting strings into substrings which form the n-gram
(e.g. “Mary eats ice” is split into “Mary eats” and “eats ice” for a bigram). A famous
project in this regard is the 1T 5-Gram Corpus by Google which analyzed 1 trillion


English words from texts and split them into frequency tables of bigrams, trigrams,
4 - grams and 5-grams (Aston & Burnard, 2022). Although the database is fruitful
for analyzing the occurrences of words over time, characters in older works looked
like others (s looked like f morphologically) and therefore the optical character
recognition algorithms are prone for errors.

**Part-of-Speech Tags**
Tagging a text corpus can have various motivations. Manning and Schütze (1999)
suggest that a tag system should exhibit “predictive features, encoding features that
will be useful in predicting the behavior of other words in the context” (p. 144).
One frequently used algorithm for assigning tags to words to indicate their part-of-
speech (POS) is the one developed by Eric Brill (1992). The algorithm employs a
supervised learning method where some words are known by a manually tagged
corpus of words and continuously transforming the tags of a word by the context
found during the processing. POS tags are often used to reduce the number of words
of interest and remove stop words.

**Levensthein Distance**
To find the distance of two sequences of characters one can use the Levensthein
distance algorithm to determine the number of insertions, deletions or substitutions
needed to get from one sequence to the other (Levenshtein, 1966). A low number
of these edits indicate a similarity in the morphology of the word and therefore the
possible similarity. Although the Levensthein distance proves to be a quite robust
algorithm to find similar word strings there are examples where distances are short,
but the semantics are totally different: bear – beer or peace – piece.

**Lexical Databases**
Lexicons are a historically important resource for understanding the meaning of
words and their etymology. A widely used lexical database for the English language
is the “WordNet” database which provides POS tags for a huge number of words
and additionally provides information about synonyms and lemmatization.





Although the lexical databases can be seen as the richest in information, they have
to be hand-assembled and are also prone to human induced error and bias.

**Word2Vec /FastText**
As the semantics of words are hard to pinpoint due to their various occurrences and
context sensitivity, the manual annotation and definition of huge corpuses of text
and their relations posed a significant problem to NLP researchers. The work of
Mikolov et al. (2013) posed a ground-breaking new unsupervised approach of
feeding large text corpora into recurrent neural networks to represent words as
vectors in large dimensional spaces. Due to the possibility to calculate distances
between high-dimensional vectors, semantically closely connected words were
found to exhibit shorter distances than words that were rather unrelated. Today
multiple huge word2vec models are open-source and can be used on edge-device
level infrastructure.

The word2vec model was a milestone in deriving semantics between words from
unsupervised learning methods, however the model to often lacked context due to
its architecture and contextual sensitive words were less well represented in these
vector spaces. This problem was tackled by a research team at Facebook who
created the “FastText” model that was more aware of contextual nuances of
meaning (Bojanowski et al., 2016). The FastText model posed significant better
processing speed and was able to handle the look up of semantics of words which
were not in the corpora. Although the FastText model is still used frequently for its
performance of calculating semantic distance between two words, it struggles to
grasp whole sentences and therefore bigger windows of context.

Although these models pose interesting use-cases for text analysis purposes,
significant critique has been stated due to the inscribed biases. Bolukbasi et al.
(2016) have shown that these models exhibited for example sexist and racist
relations between occupations and descriptive words for people. Therefore, these
models should always be employed with a certain amount of skepticism towards
the drawn relationships.


### 2.3 Text Visualization

With the advancements of text digitization the number of text data available for
computational processing rose exponentially (Michel et al., 2011). To observe these
large datasets visualizations were used to gain insights. One of the most cited works
(Card et al., 1999) on information visualization, defines it as “the use of computer-
supported, interactive, visual representations of abstract data in order to amplify
cognition” (p.7).

Risch et al. (2008) define text visualizations as a subclass of information
visualization which is historically closely associated with so-called “‘semantic
clustering’ or ‘semantic mapping’ techniques” (p. 156) and is used to “graphically
depict the overall conceptual structure of a document collection” (p.164).
Historically, text visualizations have been developed to visualize the outcomes of
NLP methods (Zanini & Dhawan, 2015). As most NLP methods are quantitative or
even statistical, visualizations that depict the outcomes of algorithmic text analysis
often use the same algorithms for display as the visualizations from the natural
sciences. This can be observed in the survey conducted by Kucher and Kerren
(2015) which shows many visualizations resemble line-, bar- or radial charts as well
as scatter plots. Prominent text visualizations are the word cloud or the DocuBurst
visualization (Collins et al., 2009; Heimerl et al., 2014). However, word clouds are
critiqued for their lack of context and oversimplification in equating relevance to
number of appearance (Dörk & Knight, 2015). Baumer et al. (2022) points out that
information visualizations in humanities related applications often obfuscate their
constructed nature and have an authority due to their symbolic heritage from the
natural science.

Nevertheless, Jänicke et al. (2015) have shown in their survey that text
visualizations are effective in the enhancement of text comprehension and analysis.
As already stated in the beginning the visual representation allows for a
significantly improved perception and enables cognitive processes to work with





abstract data structures. Therefore, the two most relevant visualization groups will
be introduced in the following sections.


**Timelines**
To understand the temporal relations between entities timelines are used as a
graphical depiction of the occurrences at specific timesteps or sequential order.
With the emergence of large text corpora’s it was possible to see the usage of
specific words and themes over time (Michel et al., 2011). Timeline visualizations
can also reveal patterns in co-appearance of characters (Liu et al., 2013),
reconstruct the temporal order of a story where the narrative order is temporally
non-linear (Kim et al., 2018) and order the appearance of phenomena along a time
axis (Schwan et al., 2019). To additionally depict a dependent quantitative
dimension, streamgraphs can be used as seen in the visualizations on the markings
and annotations in Theodor Fontane’s library by Bludau et al. (2020) (see Figure
2 ). Another important use-case is the display of text edits over time (Dias
Cantareira et al., 2023). As the meta-analysis of works of authors is also a mayor
inquiry of the digital humanities, timeline visualizations can help to show
influence and possible reproduced bias between scholars (Dörk et al., 2012) (see
Figure 3 ).


Figure 2 : Visualization of “markings” distribution found in Theodor Fontane's library
(Screenshot from Urban Complexity Lab (2019) website)






Figure 3 : Influence of Nietzsche on other philosophers (Screenshot from Marian Dörk’s
(2011) website)

**Graphs**
Data that exhibits non-hierarchical relations between entities is often displayed
through graph visualization techniques. Graphs display the relationships between
nodes as a network of visual links. The links can be directed to show various
qualitative dimensions such as dependency or temporal sequency. Ted Nelson
(Nelson, 1965) already depicted networks of text samples in his paper on the
hypertext and provided multiple prototypes and theoretical frameworks with his
“Xanadu” endeavor (Nelson, 1999). The “xanadoc“ filetype was a frontier idea for
enabling interactive and visual analysis of cross-references between text segments
(see Figure 5 ). In the digital humanities the visualization methods are often used to
show influence among authors (Dörk et al., 2012), topic networks (Dermentzi,
2021) or relationships between phenomena and entities in texts (Schwan et al.,
2019) (see Figure 3 and Figure 4 ). Semantic networks are also leveraging the graph
visualization technique to display similarity and semantic relationships among text
corpora or topics (see Figure 4 ).



Figure 4 : Semantic Network of 10 semantically nearest neighbors according to a
word2vec model (Figure reproduced from Dermentzi (2021))

_Figure 5 : Xanadoc prototype displaying links between source and synthesized document
(Screenshot from Xanadu (2016) demo website)_





## 3 Visualizing Interpretation

### 3.1 Relevance

The rise of digital computers and the corresponding emergence of the hypertext has
fundamentally altered the way how we work with texts (Hayles, 2012). New
reading habits such as F-shaped reading, skimming and scanning have evolved to
adjust mental processes to the ever rising amount of text that needs to be processed
(Nielsen, 2006). Although the effects of the internet and hypertext are often
connected to negative forms of reading and cognitive overload, some scholars
(Drucker, 2020; Manovich, 2013) have argued that the implications of the digital
could be much more fundamental for the way we interpret and interact with texts.

For instance, English literature scholar Katherine Hayles (2010) emphasizes the
possibilities of hypertext as it enables the efficient juxtaposition of multiple texts
and creation of semantic networks. These methods allow to consciously increase
complexity and alters the way we perceive text as the linear becomes non-linear.
Lev Manovich (2013) extends on this idea, by referencing Howard Rheingold’s
(2000) “tools for thought”, and declares that “computers and software are not just
“technology” but rather the new medium in which we can think and imagine
differently” (p. 13). Therefore Elena Esposito (2022) proposes that “the machine
operates as a partner making proposals that can direct interpretation in unexplored
directions” (p. 14). This novel perspective sets a focus on the hermeneutics of
display and interaction with texts, especially in the light of McLuhan’s (1966)
famous proclamation: “the medium is the message”. Although digital forms of text
are getting more dominant, qualities of paper-based texts are still consistently
reproduced on digital screens, highlighting the preference for some of its graphic
features.

Visualization plays a crucial role in the text interpretation process, as Barbara
Tversky (2011) observed the important role of visual aids for cognition and thought.
Visual language has analogous characteristics to natural language which makes it a
powerful tool for schematization and processing of meaning in natural language.


Annotations and diagrams are often found in the margins of texts for visualizing
thought and interpretation (Kalir & Garcia, 2021; Tversky, 2011). Johanna Drucker
(2020) is a key figure in the contemporary discussion around the implications of
visualizations for interpretative purposes. She argues to see interactive
visualizations as a primary way of knowledge production, as they support the
process of visual modeling of text, questions and arguments. Drucker distinguishes
between representational visualizations and non-representational visualizations.
While the first is a one-way mapping between abstract datasets and visual elements,
the second is interrogative, conditional and subjunctive, emphasizing the
ambiguous character of visualizations. In this regard text visualizations, especially
in the duty for political decision making, have been critiqued for obscuring
uncertainty in datasets, lack of context and reproduction of biases (Baumer et al.,
2022). Therefore Dörk and Knight (2015) propose to use annotations and comments
to enrich the visualizations to display its discursive qualitites. To underline this take,
a study by Chen and Chen (2014) has shown that annotations significantly boost
the capability for making inferential statements about a visualization. Kleymann
and Stange (2021) describe such discursive forms of visualizations as “hermeneutic
visualizations” which are display the interpretative nature of the dataset.

In conclusion the contemporary discussion about text interpretation and the usage
of visualization in the process is ongoing, as it is inherently interdisciplinary. The
combination of graphic design, literary studies, natural language processing and
interface design provides a promising interaction in this regard. Therefore, this
work explores the design space for combining algorithmic text analysis and human
text analysis through interactive visualizations. The work is guided by the following
research questions:


(1) How does a digital environment look like that visually augments the text
interpretation process?
(2) How can algorithmic text analysis and human-created interpretation
artefacts be visually connected in a meaningful way to support the text
interpretation process?





### 3.2 Text Visualization Workshop

To explore the usage of graphical elements during a paper-based text interpretation
process, a workshop has been conducted. To express and schematize ideas during
the text interpretation process many people fall back on drawing to model their
thought (Tversky, 2011). The drawings are often neglected in the exegesis although
they provide fruitful insight into thought patterns and structuring methods (Tversky,
2011). These drawings can be described as manually created text visualizations as
they draw on and visually encode a text. A workshop can help to focus on the
transient, process-oriented visual artefacts of text interpretation and document them
for analysis and design inspiration (Fischer, 2014). Dörk et al. (2020) have proposed
to produce tangible artefacts during the process which can be the starting point for
inspiration and discussion upon the visual dimensions of the process. Additionally,
they emphasize to provide a generative setting, to support highly expressive
visualizations. As the workshop is embedded in the greater scope of this work the
central question are:


(1) What visual encodings are used to visualize the text interpretation process?
(2) Which visual encodings can lead to confusion and ambiguous
interpretations?

To answer this questions the workshop has evaluated the produced artefacts and the
following discussion of participant observations.

**Setup**
The workshop has been conducted with 7 students with various academic
backgrounds such as design, philosophy, architecture, cognitive sciences and
engineering. The choice for different backgrounds was on purpose to promote a
wide range of different visualization approaches and techniques. It was also decided
to use a physical setup, due to the interest in symbolism and visual encoding, which
is more generative with a handful of tools. To trace the individual processes of text
interpretation all participants were asked to document their process on a DIN A3
poster. Different materials and tools were provided such as markers, pens, glue,





strings, transparent paper and scissors. It was emphasized that the text doesn’t have
to remain in its original DIN A4 format and can be disassembled with scissors. To
introduce the participants into the topic and theoretical implications of text
visualization, a short passage from a text of Elena Esposito (2022) was read
together. This text emphasized the usage of visualizations as visual provocations,
which prompt question rather than providing answers. Afterwards all participants
were asked to read and interpret a short passage of two pages from Bruno Latour
(2008) with a time limit of 60 minutes. “Interpretation” was not defined prior to
provoke a fallback on individually acquired strategies and associations.
Additionally, all participants were asked to focus on visually articulating their
thought process during the task.

**Outcomes**
During the task it was observed that almost all participants employed marking pens
for highlighting words and sections of interest. This could be attributed to the wide-
spread teaching of the technique across disciplines and even already in secondary
schools. Participants used various techniques for visually representing their
interpretation process. While the strategy for positioning elements on the poster was
diverse the usage of an almost coherent set of graphical elements was apparent. The
employment of links and arrows for pointing out relations was one of the most
common used graphical elements. Some participants used transparent paper to
create additional layers while others extracted text passages to annotate them in
isolated settings. The layout of the posters converged to especially two types: the
main text at the center with peripheral annotations and doodles or some snippets of
the text arranged to a rhizomatic mind-map. A collage of some of these artefacts
can be seen in
Figure 6.
In the following discussion participants were asked to describe and interpret the
posters of other participants. The owner of the discussed poster was asked to be
silent till the end to answer questions as well as giving insights afterwards. During
the discussion on the posters several participants pointed out a division between
one group who first read the whole text before starting to visually structure their


thoughts and another group who spontaneously annotated the text in real-time. This
division was also observed in the artefacts where the first group exhibited visually
more structured posters while the latter were more chaotic. Participants observed
different visualization layouts, which they interpreted as intended for visual
guidance.


Figure 6 : Different visualizations used for text interpretation by workshop participants.






Figure 7 : Different layouts of posters: mind-map like (left) and hierarchical separation of
main text and annotations (right).


Figure 8 : Participants engaging in discussion about the layering visualization approach
of another participant.

A much-used layout in this regard was the hierarchical separation of the main text,
at the center, surrounded by annotations and other sketches (see Figure 7 ). Another
central observation during the discussion was the uncertain entry point of some of


the posters, which raised questions about the importance of visual encoding of
sequentiality and temporality. This phenomenon was especially observed in the
real-time annotated visualizations as the space was filled spontaneously, leading to
arbitrary placed sequences of thought. Questions were raised about the usage of
conventions for depicting sequentiality such as left-to-right (in western cultures) or
top-down directions. It was also observed that some graphic elements followed a
shared symbolic language while others were more arbitrary in use and therefore
harder to decode. Especially arrows stood out as robustly unambiguous and had
been interpreted coherently by most participants while the usage of colors prompted
multiple times misinterpretations.

It was emphasized that the usage of visualization was helpful in creating analogies
and metaphors for abstract concepts, although it was debated to which extent the
visualization should be accessible for others. Some participants pointed out the non-
explicit associations for individuals which transformed visualizations into external
cognitive anchors, which were inaccessible to others. All participants agreed on the
importance of a “textual grounding” to ensure a connection between the
visualization and text segments. Experimental approaches that physically
disassembled the text into snippets or the usage of transparent paper for layering
visualizations were received positively (see Figure 6 and Figure 8 ). Participants
pointed out the increased appeal of engaging with the text due to the multimodal
possibilities which provoked associations and inspiration. Although the base for
understanding the visualizations of each participant was given, the discussion
showcased different associations according to the disciplinary backgrounds,
leading to rich discussion about the interpretation of symbolics.

Overall, the feedback on the usage of visualizations for the text interpretation
process was positive. In summary four key aspects can be extracted from the
workshop which inform the latter design process:


(1) Symbols like arrows and circles have a well-established shared foundation
of meaning.





(2) Diagrams and drawings can be used to visually remix and model artefacts
with existing knowledge.
(3) A visual encoding of sequentiality is helpful to guide through the chain-of-
thought.
(4) Visual references to the source text enhance traceability and readability of
artefacts.


### 3.3 Design Proposal

#### 3.3.1 Outline

As shown in the outcomes of the workshop text visualizations can contain
ambiguous artefacts prompting discussions about meaning. Questions arise about
the visual encoding of chain-of-thought, semiotics and non-linear structures which
emerge from the process. This emphasizes the potential of visualizations for
interpretative purposes as they provoke discussions and enable to argue visually.

As the source text is most often the central element of text interpretation, one must
look closely at the factors which influence the perception and therefore attention to
details. As shown in the related work section people concerned with the
hermeneutics of texts have developed a sensitivity for socio-historic contexts of
author and reader. To effectively analyze texts for such influences, reading
techniques have been developed to equip readers with an attention to special details.
Especially close reading has proven to be a robust strategy for analyzing texts upon
structure, topics, appearance of characters or foreign words.

Markup languages are a central component in this regard, as they shape visual
appearance of texts by defining the layout, font, structure and so on (McGann,
2015). They provoke certain engagements with texts as they provide e.g.: margins
for notes and drawings, specific fonts for readability and visual differences of
structural elements to leverage visual cognitive processes. In contrast to speech,
text is mostly displayed on a two-dimensional space – the page. Barabara Tversky
(2011) emphasizes the advantage of the page as it leverages several cognitive
processes such as the interpretation of meaning in proximity of elements and the
orientation of elements. Especially the markup of digital texts, done in the
Hypertext Markup Language (HTML), provides the possibility to interactively
change the layout and visual features of the markup on the fly.

Readers often mark and annotate texts as a means for visually exploring these
details for interpretative purposes. Graphic elements can convey meaning in this





regard and can help to think visually (Tversky, 2011). Common visual variables for
encoding information into graphic elements are position, color, shape or size
(Bertin, 1983). While these variables already provide a quite nonexhaustive amount
of combinations, Johanna Drucker (2020) adds variables like speed, attraction,
repulsion and force to the list, as modern technologies allow animation of graphic
elements which comes with its own repertoire of possible meaning.

Algorithms enable efficient calculations of sums and the lookup for specific
elements.
In this regard natural language processing can help to find relevant text passages,
which are previously requested by a human. HTML in combination with the
JavaScript programming language can visually incorporate results in the same
visual layer as the human-readable text. This allows in general the mix up of
algorithmically and human created artefacts, which can be leveraged for
interpretation.

In conclusion the key results of the workshop and the theoretical foundation
presented before are merged into a summary of qualities that produce a digital
environment for text interpretation, that visually incorporates artefacts created
during the interpretation process by human and machine. The following qualities
act as a guideline for the design process:


(1) Human and machine-readable text structure.
(2) Marking of text can be done by human and machine.
(3) Diagrams can be created by human and machine.
(4) Artefacts (markings and diagrams) are interactive.


(5) Human and machine-readable text structure.
(6) Visualization of human and machine-created text interpretation artefacts.
(7) Interactivity between human and machine-created text interpretation
artefacts.


#### 3.3.2 Design Process

- Outline of theoretical influences (Johanna Drucker)
- Interface Design

_3.3.2.1 Visual Encoding_

_3.3.2.2 Interactivity_

- Summary

### 3.4 Prototypical Iterations

## 4 Discussion

A text is not only a place for data mining and emergent patterns but an offer for
contemplation and meaning generation in contexts. Tools that offer interpretative
„gestures“ and inscriptions allow for such contemplative, meaning-making, process
enriched by individual knowledge which is set in context.

## 5 Conclusion




Bibliography


